% Preamble
\documentclass[11pt]{article}

% Packages
% 1.5 facher Zeilenabstand

\usepackage[utf8]{inputenc}
% 1.5 facher Zeilenabstand
\usepackage[onehalfspacing]{setspace}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{minted}
\usepackage{algorithm}
\usepackage{algpseudocode}




\title{MasterThesis}
\author{Tim Ruhkopf}
\date{January 2019}

\begin{document}

\begin{titlepage}

	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\center % Centre everything on the page

	\HRule\\[0.4cm]

	{\huge\bfseries Topic}\\[0.4cm]

	\HRule\\[1.5cm]
	\vfill\vfill

	\textsc{\Large twenty week graduation work as part of the examination in the course of studies Applied Statistics at the
        Georg August University G\"ottingen.}\\[1.5cm]

	\vfill\vfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\large
			submitted on: {\large\today} \\
		    author: Tim \textsc{Ruhkopf}\\
            matr.no: 21218472 \\
            email: tim.ruhkopf@outlook.de \\
			from: \textsc{Gehrden}\\

		\end{center}
	\end{minipage}

	\vfill\vfill

	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\large
			\textit{Examinors}\\
			Dr. Benjamin \textsc{S\"afken}\\
			Prof. Dr. Thomas \textsc{Kneib}

		\end{center}
	\end{minipage}

	\vfill\vfill\vfill\vfill

\end{titlepage}

    \tableofcontents
    \thispagestyle{empty} % remove pagenumber

    \clearpage
    \listoftables
    \listoffigures
    \listofalgorithms
    \addtocontents{loa}{\def\string\figurename{Algorithm}}

    \clearpage
    \setcounter{page}{1} % starte pagenumber with 1

    \clearpage
    \begin{abstract}
        This is the abstract.
    \end{abstract}

    % CONTENT
    \section{Introduction}

    \section{Literature review}
    A natural starting point into bayesian modeling is the comprehensive and partly practical guide \citep{gelman2013bayesian},

    Bayesian GAM \& Splines
    \citep{fahrmeir2013regression} chapter:

    BNN
    BAYESIAN MODELING: involves setting up a generative model including priors, likelihood model and using bayes rule to infer the resulting posterior. due to the most frequently, intractible normalizing integral, the fairly simple model formulation may require sophisticated inference strategies - particularly if the dimension of parameters increases.

    BAYESIAN INFERENCE: Variational inference \& sampling schemes. for variational - eventhough not considered in this thesis, a short introduction with reference on a more detailed seminal paper can be found in
    \citep{zhu2017big} The main idea of this approach is to find a suitable posterior approximation in form of a surrogate distribution, which is available in closed form, that reduces the Kullback leibler divergence to the desired posterior.

    % variational inference suffers in high dimensional and complex posterior settings


    SAMPLING SCHEMES

    A conceptual introduction into the use of Monte Carlo methods to approximate the unknown posterior's quantities as well as an didactically appealing progression from Importance and Rejection Sampling to Markov Chain Monte Carlo (MCMC) methods with the special and efficent cases namely Gibbs, Metropolis-Hastings and Hamiltonian Monte Carlo (HMC), regarding their methodology and suffocation in higher dimensional parameter spaces can be found with \citep{mackay2003information}.
    Considering the distribution's typical sets and their exploration in higher dimensions both \citep{mackay2003information} and \citep{betancourt2017conceptual} are good introductions to motivate efficient exploration of the typical set in the light of Monte Carlo integration.
    For MCMC based methods, special care must be taken in the construction of these samplers to ensure their convergence to the desired distribution. A seminal book reviewing the  mathematical foundations of MCMC methods in this regard can be found with \citep{brooks2011handbook}.
    \citep{neal2011mcmc}'s review on the efficient MCMC flavour HMC is a worthwhile read to grasp Hamiltonian dynamics and its properties of reversibility and preservation of volume in the context of MCMC as well as the discrete (modified) Euler and Leapfrog approximation preserving volume - provided some basic knowledge in differential equations. For the inclined reader, the line of thought on posterior inference presented on the above literature is paraphrased in chapter \ref{inference}.

    MCMC based samplers with random walk behaviour and the necessity to evaluate the entire data set at each sampling step will fail in modern machine learning applications with ever increasing amounts of data or streams of data in conjunction with increasing dimensions of the parameterspace . This is due to the inefficient exploration of the parameterspace in Gibbs \& Metropolis-Hasting, which, even if each sampling step were feasible, would bind computational resources wastefully.
    The vastly more efficient HMC in its classic form is no exception to those objections, as both the gradient computation and the Metropolis-Hastings correction to account for discretization error introduced by the Leapfrog approximations of the Hamiltonian dynamics require an evaluation of the entire data set in form of the likelihood nested in the posterior \citep{zhu2017big}.
    Most aggravating is the result found by \citep{chen2014stochastic}, that a naive on-line implementation of HMC with stochastic gradients computed on minibatches will approximate the posterior arbitrarily bad, as the preservation of energy in the Hamiltonian dynamics no longer is guaranteed. Adding friction in form of second-order Langevin dynamics to the noisy Hamiltonian system and using a reasonably sized minibatch for gradient computation establishes a CLT on the because of which, the convergence properties required by MCMC are met even with stochastic gradient approximations. \citep{chen2014stochastic}'s resulting algorithm is called Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and explicitly generalise Stochastic Gradient Langevin Dynamics (SGLD) \citep{welling2011bayesian}'s first-order Langevin dynamics.
    Building on this idea of stochastic gradients for MCMC methods, \citep{ma2015complete} deduce a complete framework to derive all samplers, that build on the idea of continuous Markov process and eventually provide samples from the target distribution. As an instance to their method, they derive Stochastic Gradient Riemanian Hamiltonian Monte Carlo (SGRHMC).

    % LESE AUFTRAG!!!


    The success of Stochastic Gradient Decent optimisers and its flavours with momentum such as ADAM \citep{ADAM} in the frequentist setting  as well as the efficient exploration properties of HMC make SGHMC and SGRHMC very promising candidates for real life Bayesian learning applications of considerable magnitude.
    A more comprehensive review on both variational and monte carlo based inference, in particular on Stochastic Monte Carlo methods in the context of Big Data problems can be found with \citep{zhu2017big}.




    \clearpage
    \section{Theory}
        \subsection{Generalized Additive Models}
        On their own, bayesian gam can be estimated using MCMC(IWLS) \citep{} % FAHRMEIR KNEIB LANG

        % CAREFULL WITH TEXT ORDER!
        % IN MY SETUP!!!!
        There is a salient connection between bayesian hidden units, bayesian regression and bayesian GAM:
        linear regression is a special case of b-hidden unit, being a single unit with identity activation. Further, they share a gaussian indepenent prior for their weights / coefficents respectively. B-GAM essentially is a special case of linear regression, but the prior on its coefficients is highly informative regarding the neighbouring structure.

        Note particularly,
        \begin{equation}
            f = Z'\gamma
        \end{equation}

        and in this szenario, the GAM itself does not carry a likelihood, but only its function approximating capabilities are passed to a joint likelihood of BNN & GAM:

        \begin{equation}
            \mu = f_{BNN} + f_{GAM}
        \end{equation}

        %the choice of likelihood is still free, making it applicable to a wide variety of Likelihoods. consider e.g. link functions for binary or multinomial regression.

        \subsection{Neural Nets}
        \subsection{Bayesian Neural Nets}
        explicit multilayer BNN formula (with latex): https://www.cs.tufts.edu/comp/150BDL/2018f/assignments/hw2.html
        for BNN with indep gaussian priors for the Hidden Units' weights & biases

        %note on model complexity & Orcam's Razor: \citepp{neal2012bayesian} Bayesian model complexity need not adjust to the amount of available training data (as frequentists would). The concept of bayesian learning via adjusted prior beliefs already incorporates Orcam's preference for simpler models to the degree that is appropriate in the specification. One rather should opt for the computationally feasible and let bayesian learning extract as much information available from the data and express the appropriate amount of care to be taken due to the resulting uncertainty. p 10


        %Note on bayesian prediction: posterior predictive p6: integral over all theta, that explain the data reaonably well and contribute to the integral.
        %ALSO: close connection of loss functions (MSE, L1, 01loss) and single value predictions based on posterior predictive distrib.: (MAP, median, mode)

        %note: motivate the use of HMC as efficient MCMC, due to fast increase in dimension.

        %NN are non-parametric functions (i.e. numerous parameters with less meaningfull interpretation)


        %MOST IMPORTANT FORMULATION OF BNN p 15: posterior predictive distribution!
        %"In the Bayesian approach to NN learning, the objective is to find the predictive distribution for the target values in a new "test" case $y^{t+1}$, given the input $x^{t+1}$ for that case and targets in the training cases." Since dist of inputs is not modeled, the likelihood in the following equation even simplifies:  ( WHY NOT MODEL distrib of X to improve on uncertainty?)

        \begin{equation} \label{predictiveProbDist}
            P(y^{(t+1)}| x^{(t+1)}, D) = \int P(y^{(t+1)}| x^{(t+1)},\theta) P (\theta| D) d\theta
        \end{equation}

        %with training data $D= (x^{(1)},y^{(1)}), \dots, (x^{(n)}, y^{(n)})$ and $P (\theta| D)$, the posterior with $\theta$ containing the weights and biases. The posterior is proportional to a simplified likelihood
        \begin{equation}
             L(\theta|  D) =  \prod_{i=1}^n P(y^{(i)}| x^{(i)}, \theta)
        \end{equation}

        %and the priors for the networks weights and biases.

        %The example of a BNN's point prediction with a squared loss is the mean of its predictive distribution. e.g. in a regression model:

        \begin{equation}\label{predictiveSingleValueSQLoss}
            \hat{y}_k^{(n+1)} = \int f_k(x^{(n+1)}, \theta) P(\theta| D) d\theta
        \end{equation}
       % where $f_k$ is the NN's prediction based on the parameter set $\theta$. Note, that it is not $f(x|\theta)$, as $f$ is a function not a distribution

        %according to neal p 16: mackay has tried gaussain priors with reasonable results. particularly with the weights'priors variances to be hyperparameters!, allowing the model to adapt the smoothness, based on the information contained in the data
        %BNN note on Monte Carlo integration:
        %the objective of deterimining the posterior predictive distribution eq. (\ref{predictiveProbDist}) requires evaluating the expectation of the function $a(\theta)= f_k(x^{(n+1)}, \theta)$:
        \begin{equation}
            E(a) = \int a(\theta) P(\theta) d\theta
        \end{equation}
        %which in turn can be used for determining the objective in  the single value prediction of $y^{(n+1)}$ in the regression example with squared error loss eq. (\ref{predictiveSingleValueSQLoss})
        p26



        %Note on MCMC (effective samples) \& how to determine autocorrelation: on chain


        % GIBBS SAMPLING MESSY in NN , but Gibbs sampling is one component of the hybrid Monte Carlo algorithm, which can be used for neural networks for hyperparam

        % detailed balance condition proof for MCMC neal: p31 also with a hint towards energy!

        % (1) remainder of neal: apropriate prior choice!
        % (2) prediction using HMC
        % (3) performance evaluation of bnn with hmc. (not limiting complexity by amount of data.) Medium sized data. WHAT ABOUT MORE EFFICIENT HMC with streaming

        % INFINITE PRIOR NETWORKS p37 GAUSSIAN LIMIT PRIOR
        % core idea: for a single point x^{(1)} elaborate the conditional prior distribution of all "reasonable" functional values (=output units) f_k(x^{(1)} = b_k + \sum_j^H v_{jk} h_j(x) with h_j(x) = tanh(a_j + \sum_i^I u_{ij} x_i)
        % where b_k ~ N(0,\sigma_b) , v_{jk} ~ N(0, \sigma_v) and respective for a and u.
        %implied by all associated priors for weights p(0, \sigma_w and biases. given some number of Hidden Units H.
        % it follows with CLT for infinite H that f_k(x^{(1)} ~ N(0, \sigma_b^2 + H \sigma_v^2 E[h_j(x{(1)})^2]).
        % setting \sigma_v = \omega_v H^-(1/2) for increasing H with some fixed \omega_v makes the variance of f_k(x^{(1)} bounded.
        % Afterwards, the joint prior distribution for function f_k over a number of fixed points x^{(1)} \dots x^{(n)} can be determined and in limit actually is a gaussian process. To do this, merely the Covariance of the function evaluated at the points must be determined: E[f_k(x^({p)} f_k(x^({q)}] = \sigma_b^2 + \omega_v^2 E[h_j(x^({p)} h_j(x^({q)}]. the letter Expectation ( E[h_j(x^({p)} h_j(x^({q)}] being the same for all observations. Note whilest the prior for every single f_k is the same, their mutual covariances in the joint prior are not! this reduces the space of "reasonable" functions under the prior, given some number of fixed  observations.

        % WIRKLICH GUTE ZUSAMMENFASSUNG VON HMC p67.

        %BNN formulation p 71:
        Posterior of weights and biases $\theta$ that may be dependent on a set of hyperparameters $\gamma$ and the trainingset $D= (x^{(1)},y^{(1)}), \dots, (x^{(n)}, y^{(n)})$ .
         \begin{equation} \label{BNNPosterior1}
            P(\theta, \gamma| D) \propto \prod_{c=1}^n\left[P(y^{(c)}|x^{(c)}, \theta, \gamma \right] P(\theta|\gamma)P(\gamma)
        \end{equation}

        Predictions are made by integration with respect to this posterior distribution.

        \begin{equation}\label{BNNPosteriorPredictive1}
            P(y^{(t+1)}| x^{(t+1)}, D) = \int P(y^{(t+1)}| x^{(t+1)},\theta, \gamma) P (\theta , \gamma| D) d\theta d\gamma
        \end{equation}

        single valued prediction, minimizing the squared loss is the mean of the predictive distribution. If y's conditional distribution is defined to have mean by the networks output:
        \begin{equation}\label{predictiveSingleValueSQLoss1}
            \hat{y}^{(n+1)} = \int f(x^{(n+1)}, \theta) P(\theta, \gamma| D) d\theta d\gamma
        \end{equation}

        %In the MOnte Carlo approach, these integrals, which take the form of expectations of functions with respect to the posterior distribution, are approximated by the average value of the function over a sample of values from the posterior

        % sampling scheme: alternate hyperprior sampling using gibbs with HMC for network parameters (conditional on hyperparams).

        %p78 Calculation of thederivatives of E with respect to the q i is required in order to perform the leapfrog iterations; these derivatives can be found by the usual "backpropagation" method
    \clearpage
    \section{Experiment}
        Even though more generally available, this analysis confines itself to
        \begin{align}
            \eta &= f_1(z_1) + f_2(z_2) + f_{12}(z_1 , z_2) \\
                 &= Z_1 \gamma_1 + Z_2 \gamma_2 + Z_{12} \gamma_{12}
        \end{align}
        with Z being appropriate basis representations.
        \subsection{Data generating process}
            \subsubsection{Sampling main effects} \label{maineffect}
            To sample arbitrarily complex, smooth functions $f(x)$, on a given support $x \in {\rm I\!R}$, Bayesian P-splines following their formulation in \citep{fahrmeir2013regression} can easily be employed:

            The B-spline expansion at some continuous z
            \begin{equation} \label{Bspline}
                f(z) = z'\gamma = \sum_{j=1}^d B_j^l (z)\gamma_j
            \end{equation}
            with $l$, the basis degree and $d$ the
            Assuming smoothness of $f$ in form of the conditional prior density
            \begin{equation}
                p(\gamma_j|\gamma_{j-1}, \dots, \gamma_1) \propto N(\gamma_{j-1}|\tau^2)
            \end{equation}

            abiding first order markov property and implying the joint Normal prior distribution is a Random Walk of order 1.
            \begin{equation} \label{1dprior}
                p(\gamma|\tau) = p(\gamma_1)\frac{1}{(2\pi\tau^2)^{(d-1)/2}}exp\left( -\frac{1}{2\tau^2} \gamma K \gamma \right)
            \end{equation}

            which, given some initial state distribution for $\gamma_1$ and precision Matrix $\frac{1}{\tau^2}K = D_1'D_1$ with $D_1$, the first order difference matrix.  Therefore, sampling $\gamma$ from its prior distribution given some fixed $\tau$, an initial state $\gamma_1$ and assuming evenly distributed placement of the basis functions, is exactly the vector of cumulatively added Gaussian noise given this particular $\tau$. Evaluating (\ref{Bspline}) given this draw of $\gamma$ for some $z$, is evaluating the random smooth function $f$.



            \subsubsection{Sampling 2D interaction surfaces}

            %NEW VERSION WITH APPROPRIATE GMRF:
            note the role of a radius $\delta$ to define the discrete neighborhood structure - an implication on the correlation structure - a somewhat squared circle of influence: Consider a discrete grid of coordinates at which the GMRF is sampled. imposing the discrete neighborhood structure $s \sim r$ on the actual distance constrained by a radius $d(r,s)<\delta$ allows to easily introduce anisotropy by contracting the distance $d$

            K HIGHER ORDERS : FAHRMEIR KNEIB LANG p509



%            The above procedure of sampling smooth functions can be generalised to a Random Walk in a second dimension in various ways. One way to do so is the generalisation via Tensorproduct  B-Splines (Te-Splines), whose coefficients $\gamma_{ij}$ location variables following a Gaussian Random Field (GRF)
    %
    %            \begin{equation} \label{tespline}
    %                f(z_1, z_2) = \sum_{j=1}^{d_1}\sum_{r=1}^{d_2} B_j^l (z_1) B_r^l (z_2) \gamma_{jr}
%            \end{equation}
%
%            support $x \in {\rm I\!R^2}$
%
%
%            Stationary process,
%
%            The Bayesian model interpretation of an GRF closely resembles \ref{1dprior} with an appropriate $K$
%
%            GRFs can be sampled following the algorithm based on \citep{pichot2016algorithms}.
%
%
%        \begin{algorithm}
%        \caption{General algorithm for random field generation}
%        \label{sampleGMRF}
%        \begin{algorithmic}[1]
%        \Procedure{generate sample}{$X_{n  m}$} with $m \in \{1,2,3\}$ \label{dim}
%            \State \label{corr} calculate $\{R\}_{jk} = Cov(|x_j - x_k|_2) = Cov(h)$
%            \State \label{fac2} factorise $R=BB^T$
%            \State draw $\theta \sim N_n(0, I_n)$
%            \State obtain realisation $B^T\theta$
%        \EndProcedure
%        \end{algorithmic}
%        \end{algorithm}
%
%        GAUSSIAN KERNEL: AUTOCOVARIANCE FUNCTIONS????
%
    %           Line \ref{dim} of the above algorithm hints towards its applicability in one, two and three dimensions. This analysis confines itself to continous interaction effects of two covariates Further note, line \ref{fac2} considers euclidean distance in an isotropic setup and can readily be extended to anisotropy if instead the rotated and prolonged distance according to \citep{fahrmeir2013regression} is used:
%
%            \begin{align}
%                h = \sqrt{(x_j -x_k)^T R(\psi)^T D(\delta) R(\psi) (x_j-x_k)}
%            \end{align}
%            \begin{align}
%                R(\psi) = \begin{pmatrix}
%                        \cos{(\psi)} & \sin{(\psi)} \\
%                        -\sin{(\psi)} &  \cos{(\psi)}
%                    \end{pmatrix} \qquad
%               D(\delta) = \begin{pmatrix}
%                        \delta^{-1} & 0 \\
%                        0 & 1
%                    \end{pmatrix}
%            \end{align}
%
%            with $\psi \in [0,2\pi]$, $\delta \geq 1$. If $\psi=0$ and $\delta = 1$, the special case of isotropy is recovered. This extension allows for directed assumptions of smoothness, as the distance is contracted and twisted in space.
%            Line \ref{fac2} requires special attention. Considering, the Te-Splines are placed on an equally spaced grid $X_{n2}$, $R$ is a structured matrix. Aside of choletzky and eigendecomposition, \citep{pichot2016algorithms} proposes a circulant embedding approach, heavily exploiting this structure for significant performance gains in fatorization of large grids. For the purpose of this analysis, despite the elegance of circulant embedding, the eigendecomposition is employed, as the surfaces complexity can be sufficiently altered using varying degrees of smoothness, rather than increasing the set of discrete points in GMRF and the increased domain of Te-Spline.
        \subsection{Results}


    \section{A note on MCMC posterior inference} \label{inference}
        Bayesian models are generally of the form

        \begin{equation}
            p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta)}
        \end{equation}
        with $p(y|\theta)$, the likelihood model of the data and $p(\theta)$ the prior model before seeing the data, most often suppressing the denominator's normalising constant with proportionality. Both the prior and likelihood model can be evaluated. However, the resulting posterior distribution $p(\theta|y)$ generally is not analytically available, preventing direct analytical inference on the quanities of interest. Instead, it is required to resort to numerical approximations of the posterior.

        \subsection{Monte Carlo Integration}
        As \citep{mackay2003information} states, the main goal of posterior inference on $p(x)= \frac{1}{Z} w(x)$ with unknown normalising constant $Z$ is to numerically approximate the expectation of a functional $f(x)$

        \begin{equation}\label{montecarlo}
            E(f(x)) = \int x f(x) p(x) dx
        \end{equation}

        particularly, the functionals allow to evaluate properties such as mean, mode, quantiles or variances based on i.i.d samples from the desired posterior distribution. Following \citep{brooks2011handbook} chp. 1.7, this heavily exploits the CLT on an estimator of the above integral, e.g. in the univariate case:

        \begin{equation}
            \hat{\mu} = \frac{1}{n}\sum_{i=1}^n f(x_i)
        \end{equation}

        \begin{equation} \label{clt}
            \hat{\mu} \sim N\left(\mu, \frac{\sigma}{n}\right)
        \end{equation}

        with $\sigma = var(f(X))$, $\mu$ and $\hat{\mu}$ being the expectation in (\ref{montecarlo}) and the  estimator of the same respectively.
        The major difficulties that arise are at least twofold: first, generally the posterior is not readily available for sampling, as the normalising constant is unknown. Secondly, the curse of dimensionality renders it infeasible to obtain independent representative samples in reasonable amounts of time, as could be obtained for instance by importance or rejection sampling. The latter is implied by the condensing typical set, i.e. an in dimension $d$ increasingly small area, in which most of the distribution's volume resides, thereby being most informative for the approximated integral in (\ref{montecarlo}) (see \citep{mackay2003information} chp. 4.4).

        \subsection{Markov Chain Monte Carlo}
        To overcome the problem of dimensionality, Markov Chain Monte Carlo Methods introduce dependence between the samples using a carefully constructed Markov chain, whose stationary distribution is in fact the posterior distribution in order to explore the typical set (compare \citep{betancourt2017conceptual} chp. 2.4). Thereby, once converged into this stationary distribution, highly informative but (potentially) strongly correlated samples are obtained from the posterior distribution.
        Essentially, the MCMC's Markov Chain is designed to asymptotically spend the right amount of time - which is proportional to the volume of the area under the desired posterior to effectively reflect samples drawn from it. Given the CLT in (\ref{clt}) can be established despite the auto-correlation, $\sigma$ is expanded by this structure, motivating thinning on the obtained sample sequence in order to gain close to independent samples from the posteriors typical set. The length of auto-correlation drives the "effective sample size", and thereby the computational effort to obtain independent samples from the posterior.
        The defacto random walk exploration of the sampling space contrasts importance and rejection sampling schemes in a distinct way: both employ some form of weighting and rejection schemes of uniform independent proposals respectively. Both essentially try to explore the entire space uniformly and adjust the samples for their relative importance in the sense of (\ref{montecarlo}) either with a weighting scheme or stochastically. However, with the curse of dimensionality it becomes increasingly infeasible to hit - and gain sufficiently many samples from the typical set (see \citep{mackay2003information} chp. 29.2 -29.3).
        The random walk exploration driven by a Markov Chain's transition probability becomes apparent denoting the special MCMC case of Metropolis-Hastings \citep{} by the form of \citep{roberts1999note} chp. 2 with some abuse of notation.

        \begin{equation}
            P_{ij} = \begin{cases}
                        q(i|j)\alpha(i,j) & \text{if } i \neq j \\
                        \int q(i|k)[1-\alpha(i,k)] dk & \text{if } i = j
                        % fehlt q(ii)?
            \end{cases}
        \end{equation}

        with $q(i|j)$ being a proposal density and $\alpha(i,j) = min\left\{1, \frac{p(i)q(i|j)}{p(j)q(j|i)} \right\}$, the accepance probability. This perspective allows to establish CLT based on Markov chain theory under certain conditions (for an enriched discussion on Markov chain conditions and their rate of convergence for CLT consult e.g. \citep{jones2004markov}).

        % GIBBS SAMPLING IS SPECIAL CASE OF MCMC
        % I will have to explain GIBBS sampling in short for hyperparameter tuning

        % DETAILED BALANCE
        Even though not sufficient and not necessarily required for establishing CLT on its own, one condition, namely detailed balance for all states i,j i.e. reversibility\footnote{Reversibility of a Markov Chain is requiring the joint distribution of forward and backward pass of a sequence to be the same w.r.t the initial distribution. (see \citep{brooks2011handbook} chp.1.5)} of the Markov Chain stresses the chain's convergence to the desired stationary distribution which by construction is the intended posterior distribution. Note that reversibility implies stationarity.

        QUELLE ANGEBEN:
        %https://ermongroup.github.io/cs323-notes/probabilistic/mh/

        % NEAL DISS:
        %Note on MCMC: (detailed balance, reversibility \& erdogedicity: Invariance (i.e. a persistent stationary distribution once it is met - (=> $P(\theta')= \int T(\theta'|\theta)P(\theta) d\theta$) with respect to Q is implied by the stronger condition of detailed balance. A chain satisfying detailed balance is said to be reversible . A Markov chain that is ergodic has a unique invariant distribution, its equilibrium distribution, to which it converges from any initial state. p 27.
        \begin{align*}
        \label{MHdetailed}
            w(i)q(i,j)\frac{w(j)q(j,i)}{w(i)q(i,j)} &= w(j)q(j,i) 1 \\
            w(i)q(i,j)\alpha(i,j) &=  w(j)q(j,i)\alpha(j,i)         \\
            p(i)P_{ij} &= p(j)P_{ji}
        \end{align*}

        given $\alpha(i,j) \leq 1$  and consequently by definition, the reverse acceptance probability must be $\alpha(j,i) = 1$, since the the inverse of a fraction that was smaller or equal to one, is necessarily larger or equal to one and by definition of the acceptance probability i n MH is cutoff at one.
        The last step is obtained by multiplying both sides with $\frac{1}{Z}$ and applying the definition of the transition kernel.
        Note however that the assessment of convergence is yet another unresolved issue.

        % make sure to tune it right: Proposal covariance structure - dependence of the parameters -
        % to small variance leads to high dependence in the transitions - reducing the effective sample size due to autocorrelation but ensuring to stay in the typical set.
        % to large variance may lead to jumps out of the typical set with low density, yielding high rejection rates, requiring many draws to explore the typical set.

        \citep{mackay2003information} Metropolis-Hastings' performance in high dimensions is dependent on step size $\epsilon$ (in reference to the proposals standard deviation in the particular dimension direction) which must be considered in the the lengthscale of the posterior,  % check this: p294, chp 29.4


        % Harris recurrence, erdogedicity, independence of initial state distribution to converge to the posterior distribution


        \subsection{Hamiltonian Monte Carlo}
        Metropolis-Hastings and various extensions to it already provide algorithmic solutions to many bayesian model formulations.
        To motivate and paraphrase HMC, follow  \citep{neal2011mcmc}'s comprehensive review of the method originally proposed by % Duane, Kennedy, Pendleton, and Roweth 1987 (hybrid monte carlo)
        closely:

        \begin{quote}
            \it{"The Hamiltonian Monte Carlo method alternates simple updates for [\dots] momentum variables with Metropolis up-dates in which a new state is proposed by computing a trajectory according to Hamiltonian dynamics, implemented with the leapfrog method. A state proposed in this way can be distant from the current state but nevertheless have a high probability of acceptance. This bypasses the slow exploration of the state space that occurs when Metropolis updates are done using a simple random-walk proposal distribution."}   \normalfont     (\citep{neal2011mcmc}: 2)
        \end{quote}

        First and foremost, see that HMC in fact can be expressed as a particular Markov Chain constructed by Hamiltonian dynamics, rendering it a special case of MCMC and directing the discussion on its convergence properties to those of this particular Markov Chain.

        \begin{equation}
            P_{ij} =
        \end{equation} % note that acceptance probability comes from bounded approximation error due to leapfrog

        The Hamiltonian $H$ and its associated dynamics incorporate knowledge about the posterior. The Hamiltonian dynamics describe a system of constant energy $H(q,p) = U(q) + K(p)$ consisting of potential energy $U(q)$, representing the log posterior and kinetic energy $K(p)$, usually a Gaussian kernel. The differential equations relate both energies to fullfill energy conservation (i.e. conservation of the Hamiltonian) during the evolution in time:

        \begin{align}
            \frac{dq_i}{dt} &= \frac{\partial H}{\partial p_i} =  \frac{\partial K}{\partial p_i} \label{hamiltoneq1} \\
            \frac{dp_i}{dt} &= -\frac{\partial H}{\partial q_i} = -\frac{\partial U}{\partial q_i}  \label{hamiltoneq2}
        \end{align}

        with $i = 1, \dots, d$, where $d$ is the number of dimensions of the parameter space.

        Borrowed from physical applications, the canonical distribution for the Hamiltonian as energy function is used to describe the joint distribution of potential energy and momentum:

        \begin{equation}\label{canonical}
            P(q,p) = \frac{1}{Z}exp(-H(q,p)/T) = \frac{1}{Z}exp(-U(q)/T)exp(-K(p)/T)
        \end{equation}

        with temperature $T=1$ of the system, $Z$ a normalising constant. Note that the resulting canonical distributions of $U$ and $K$ are independent. In consequence, $K$ can be chosen arbitrarily, often assuming $K(p) = p^TM^{-1}p$ i.e. to be of Gaussian form by default with diagonal covariance. % more refined: adjusted HMC: vorläufige runs um hier ein besseres estimate der korrelation zwischen den parametern zu bekommen - und damit wiederum eine bessere exploration im parameter space (vgl.: mckay - slow MCMC exploration)

        HMC is a two step algorithm, first sampling a new momentum independently from the implied Gaussian distribution in (\ref{canonical}), such that $H(q, p^{new})$ is a new energy level. Given this new energy level, following a trajectory in the system of (\ref{hamiltoneq1}) \& (\ref{hamiltoneq2}) and constant energy $H$, effectively follows the bound of the levelset of the underlying posterior distribution to the next proposal $(q', p')$. In this sense, sampling a new momentum is a move between levelsets. % and or determines the (random) length of the trajectory ??

        % properties of this transition: leaving the canonical distribution invariant, meaning: ...

        The discrete leapfrog approximation of the trajectory is a refined and more stable version of Euler's method, following the implied gradient by (\ref{hamiltoneq1}) \& (\ref{hamiltoneq2}) for some small distance $\epsilon$ starting in a point $(q,p)$ to some intermediate $(q^*, p^*)$. The procedure is repeated $L$ times to arrive at the proposal  $(q', p')$ with close to constant $H$. In contrast to Euler's method, leapfrog splits the update of $(q,p)$ and distributing the step length $\epsilon$ across these steps:
        %leapfrog steps both $$\epsilon$ and $L$ are drawn random to avoid exact periodicity

        \begin{align} \label{leapfrog}
            p(t + \epsilon/2) &= p_i(t) -\frac{\partial U}{\partial q_i} (q(t)) \\
           q_i(t+\epsilon) &= q_i(t) + \epsilon\frac{p_i(t+ \epsilon/2)}{m_i} \label{leapQupdate} \\
           p_i(t+\epsilon) &= p_i(t+ \epsilon/2) - (\epsilon/2) \frac{\partial U}{\partial q_i} (q(t +\epsilon))
        \end{align}

        i.e. starting at $(q,p)$, the partial derivative of $U$ at $q$ is evaluated to update the momentum to the extent $(\epsilon/2)$, before the new state of $q$ is determined under this partially updated $p$ for the full length of $\epsilon$. At last, to arrive at the final proposal $(q', p')$, $p$ must be updated by the remaining part of the step, to get to an approximately preserved state of $H$.
        To account for the small approximation error, an acceptance rate is introduced in order to leave the properties of the original transition almost unchanged and correct for the bias.

        \begin{equation} \label{hmcaccept}
            \alpha((q,p),(q', p')) = min\left[1, exp(-H(q',p') + H(q,p))\right]
        \end{equation}

        this eq. makes the approximation error in terms of energy apparent and incorporates it into the Markov Chain.
        Noteworthy, leapfrog avoids divergence  and spiralling into the origin for the trajectory for non pathologic structures (see \citep{betancourt2017conceptual} chp. 6), which put themselves forward for diagnostics. Betancourt notes further, that this correction scheme also scales very well in higher dimensions, as leapfrog ocilitates near the exact energy level set such that the acceptance probability will deteriorate only negligibly %- which may become a major problem in MCMC in higher dimensions SOURCE??


      %  The detailed balance condition in \citep{neal2011mcmc} chp. 3.2 sheds light on the connection of HMC to HM:

       % \begin{equation}
       %    \frac{V}{Z}exp(-H_{A_k})min\left[1,exp(-H_{B_k} + H_{A_k})\right] = \frac{V}{Z}exp(-H_{B_k})min\left[1,exp(-H_{A_k} + H_{B_k})\right]
        %\end{equation}

        %Particularly $min[ . ]$ corresponds to the acceptance probability and $\frac{V}{Z}exp(-H_{A_k})$ corresponds to the probability of state $A_k$ under the canonical distribution

        \citep{neal2011mcmc} shows that Hamiltonian dynamics are reversible, easing the proof of MCMC updates using Hamiltonian dynamics leaving the desired distribution invariant. The proof requires the Markov chain transitions, which in case of HMC are generated by Hamiltonian dynamics, to be reversible.

        % what is the role of volume for MCMC? - no adjustment of acceptance rate to keep it a proper transition?
        % The detailed balance condition for HMC can be found with chp. Proof that HMC leaves the canonical distribution invariant (NEAL) Note that with the momentums distribution being independent of $U$, this is the proof for p(x) being the stationary distribution of the underlying process, ignoring p

        % VERY IMPORTANT -----------------------------------------
        % (1) How BIJECTORS WORK for e.g. Variance (SOURCE??)
        % (2) HIERARCHICAL MODELS (NEAL- and performance)
        % (3) how gradient based flavours of HMC work - IF I USE THEM -- SEE Paper (chen) stochastic gradient HMC




    \clearpage
    \section{Conclusion}


    % BIBLIOGRAPHY & APPENDIX
    \clearpage
    \bibliographystyle{aa}
    \bibliography{sources.bib}

    \clearpage
    \section{Appendix}
        \begin{minted}[
            frame=lines,
            framesep=2mm,
            baselinestretch=1.2,
            bgcolor=lightgray,
            fontsize=\footnotesize,
            linenos
        ]
            {python}
            import numpy as np
        \end{minted}

    \clearpage
    \section{Statutory declaration}
    Ich versichere, dass ich die Arbeit selbst\"andig und ohne Benutzung anderer als der angegebenen Hilfmittel angefertigt habe. Alle Stellen, die w\"ortlich oder sinngem\"a\ss{} aus Ver\"offentlichungen oder anderen Quellen entnommen sind, sind als solche kenntlich gemacht.Die schriftliche und elektronische Form der Arbeit stimmen \"uberein.

       \vspace{2.5cm}

\begin{tabular}{p{0.3\textwidth}cp{0.3\textwidth}}
  \cline{0-0} \cline{3-3} \\
  Ort, Datum & & Ruhkopf
\end{tabular}

\end{document}